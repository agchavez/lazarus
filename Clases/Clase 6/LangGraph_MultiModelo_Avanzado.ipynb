{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ LangGraph Avanzado - Multi-Modelo y Arquitecturas Complejas\n",
        "## Clase 6: Llevando LangGraph al Siguiente Nivel\n",
        "\n",
        "### üéØ Objetivos de esta clase:\n",
        "\n",
        "1. **Usar diferentes modelos en diferentes nodos** seg√∫n la complejidad de la tarea\n",
        "2. **Integrar Ollama con Docker** para modelos locales\n",
        "3. **Enrutamiento inteligente** entre modelos (GPT-4 para tareas complejas, GPT-3.5 para simples)\n",
        "4. **Persistencia avanzada** con checkpoints\n",
        "5. **Arquitecturas de agentes** real-world\n",
        "\n",
        "### üí° Caso de uso real:\n",
        "\n",
        "Imagine una empresa de construcci√≥n donde:\n",
        "- **Preguntas simples** (stock, horarios) ‚Üí Modelo r√°pido/barato (GPT-3.5 o Llama local)\n",
        "- **An√°lisis t√©cnicos** (especificaciones, comparaciones) ‚Üí Modelo potente (GPT-4)\n",
        "- **C√°lculos y validaciones** ‚Üí Modelo local sin costo (Llama)\n",
        "- **Respuestas finales** ‚Üí Modelo premium para calidad\n",
        "\n",
        "**Ahorro de costos:** Usar GPT-4 solo cuando realmente se necesita puede reducir costos hasta 90%\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ PARTE 0: Instalaci√≥n y Setup\n",
        "\n",
        "### Paquetes necesarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Paquetes instalados\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain-openai langchain-ollama langchain-core langchain-community python-dotenv -q\n",
        "\n",
        "print(\"‚úÖ Paquetes instalados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuraci√≥n lista\n",
            "\n",
            "üìã Requisitos:\n",
            "  1. OpenAI API Key configurada\n",
            "  2. Docker instalado (para modelos locales)\n",
            "  3. Ollama corriendo en http://localhost:11434\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise ValueError(\"‚ö†Ô∏è OPENAI_API_KEY no encontrada\")\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n lista\")\n",
        "print(\"\\nüìã Requisitos:\")\n",
        "print(\"  1. OpenAI API Key configurada\")\n",
        "print(\"  2. Docker instalado (para modelos locales)\")\n",
        "print(\"  3. Ollama corriendo en http://localhost:11434\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üéØ PARTE 1: M√∫ltiples Modelos en un Grafo\n",
        "\n",
        "## El Problema:\n",
        "\n",
        "En un sistema real, NO todas las tareas requieren el mismo modelo:\n",
        "\n",
        "```\n",
        "Tarea Simple (\"¬øHorario?\") ‚Üí GPT-3.5 Turbo ($0.0005/1K tokens) ‚úÖ\n",
        "                             vs\n",
        "                             GPT-4 ($0.03/1K tokens)        ‚ùå (60x m√°s caro)\n",
        "\n",
        "Tarea Compleja (\"Analizar especificaciones t√©cnicas\") ‚Üí GPT-4 ‚úÖ\n",
        "```\n",
        "\n",
        "## La Soluci√≥n: Router Inteligente\n",
        "\n",
        "Crearemos un grafo que:\n",
        "1. **Clasifica** la complejidad de la pregunta\n",
        "2. **Enruta** a diferentes modelos seg√∫n complejidad\n",
        "3. **Optimiza** costos autom√°ticamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Configurar M√∫ltiples Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programs\\Anaconda\\envs\\curos-ia\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3517: UserWarning: Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
            "  if await self.run_code(code, result, async_=asy):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modelos configurados:\n",
            "\n",
            "   üîµ Classifier: gpt-3.5-turbo (r√°pido y barato)\n",
            "   üü¢ Simple: gpt-3.5-turbo (tareas b√°sicas)\n",
            "   üü° Advanced: gpt-4o-mini (an√°lisis complejos)\n",
            "   üî¥ Premium: gpt-4o (respuestas cr√≠ticas)\n",
            "\n",
            "üí∞ Comparaci√≥n de costos (por 1M tokens input):\n",
            "   gpt-3.5-turbo: $0.50\n",
            "   gpt-4o-mini:   $0.15\n",
            "   gpt-4o:        $2.50\n",
            "\n",
            "   Ahorro potencial: Hasta 5x usando routing inteligente\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# üìå M√öLTIPLES MODELOS PARA DIFERENTES TAREAS\n",
        "\n",
        "# 1. Modelo BARATO Y R√ÅPIDO para clasificaci√≥n inicial\n",
        "llm_classifier = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    model_kwargs={\"seed\": 42}  # Para reproducibilidad\n",
        ")\n",
        "\n",
        "# 2. Modelo ECON√ìMICO para tareas simples\n",
        "llm_simple = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# 3. Modelo POTENTE para tareas complejas\n",
        "llm_advanced = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # GPT-4 Turbo es m√°s barato que GPT-4 original\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "# 4. Modelo PREMIUM para respuestas finales cr√≠ticas\n",
        "llm_premium = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelos configurados:\")\n",
        "print(\"\\n   üîµ Classifier: gpt-3.5-turbo (r√°pido y barato)\")\n",
        "print(\"   üü¢ Simple: gpt-3.5-turbo (tareas b√°sicas)\")\n",
        "print(\"   üü° Advanced: gpt-4o-mini (an√°lisis complejos)\")\n",
        "print(\"   üî¥ Premium: gpt-4o (respuestas cr√≠ticas)\")\n",
        "\n",
        "print(\"\\nüí∞ Comparaci√≥n de costos (por 1M tokens input):\")\n",
        "print(\"   gpt-3.5-turbo: $0.50\")\n",
        "print(\"   gpt-4o-mini:   $0.15\")\n",
        "print(\"   gpt-4o:        $2.50\")\n",
        "print(\"\\n   Ahorro potencial: Hasta 5x usando routing inteligente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Definir el State y Nodos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ State definido con 7 campos para tracking completo\n"
          ]
        }
      ],
      "source": [
        "# State que pasa entre nodos\n",
        "class StateMultiModelo(TypedDict):\n",
        "    pregunta: str                    # Pregunta del usuario\n",
        "    complejidad: str                 # \"simple\", \"complejo\", \"critico\"\n",
        "    contexto_tecnico: str            # Informaci√≥n t√©cnica extra√≠da\n",
        "    respuesta: str                   # Respuesta final\n",
        "    modelo_usado: str                # Qu√© modelo se us√≥\n",
        "    tokens_estimados: int            # Estimaci√≥n de tokens usados\n",
        "    requiere_validacion: bool        # Si necesita validaci√≥n humana\n",
        "\n",
        "print(\"‚úÖ State definido con 7 campos para tracking completo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ 4 nodos definidos, cada uno con su modelo especializado\n"
          ]
        }
      ],
      "source": [
        "# üéØ NODO 1: Clasificador de Complejidad\n",
        "def clasificar_pregunta(state: StateMultiModelo) -> StateMultiModelo:\n",
        "    \"\"\"\n",
        "    Usa GPT-3.5 (barato) para clasificar la complejidad de la pregunta.\n",
        "    Esto determina qu√© modelo usaremos despu√©s.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç [CLASSIFIER] Analizando: '{state['pregunta']}'\")\n",
        "    \n",
        "    prompt = f\"\"\"Clasifica la complejidad de esta pregunta en una de estas categor√≠as:\n",
        "\n",
        "SIMPLE: Preguntas b√°sicas sobre horarios, ubicaci√≥n, contacto, stock simple.\n",
        "Ejemplos: \"¬øA qu√© hora abren?\", \"¬øTienen rotomartillos?\", \"¬øD√≥nde est√°n ubicados?\"\n",
        "\n",
        "COMPLEJO: Requiere an√°lisis t√©cnico, comparaciones, c√°lculos, recomendaciones.\n",
        "Ejemplos: \"¬øCu√°l equipo es mejor para demoler concreto armado?\", \"Necesito comparar especificaciones\"\n",
        "\n",
        "CRITICO: Decisiones importantes, cotizaciones grandes, consultas legales/contractuales.\n",
        "Ejemplos: \"Necesito cotizaci√≥n para proyecto de HNL 100K\", \"¬øQu√© garant√≠as ofrecen?\"\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Responde SOLO con: SIMPLE, COMPLEJO o CRITICO\"\"\"\n",
        "    \n",
        "    response = llm_classifier.invoke([HumanMessage(content=prompt)])\n",
        "    complejidad = response.content.strip().upper()\n",
        "    \n",
        "    print(f\"   Complejidad detectada: {complejidad}\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"complejidad\": complejidad,\n",
        "        \"tokens_estimados\": len(state['pregunta']) // 4  # Estimaci√≥n rough\n",
        "    }\n",
        "\n",
        "# üü¢ NODO 2: Responder Simple (GPT-3.5)\n",
        "def responder_simple(state: StateMultiModelo) -> StateMultiModelo:\n",
        "    \"\"\"\n",
        "    Usa GPT-3.5 para preguntas simples.\n",
        "    R√°pido, barato, suficiente para consultas b√°sicas.\n",
        "    \"\"\"\n",
        "    print(\"\\nüü¢ [SIMPLE MODEL] Procesando con GPT-3.5...\")\n",
        "    \n",
        "    prompt = f\"\"\"Eres un asistente de CONCESA, empresa de renta de equipos de construcci√≥n.\n",
        "Responde esta pregunta de forma breve y profesional.\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "    \n",
        "    response = llm_simple.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    print(f\"   ‚úÖ Respuesta generada ({len(response.content)} chars)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"respuesta\": response.content,\n",
        "        \"modelo_usado\": \"gpt-3.5-turbo\",\n",
        "        \"requiere_validacion\": False\n",
        "    }\n",
        "\n",
        "# üü° NODO 3: Responder Complejo (GPT-4o-mini)\n",
        "def responder_complejo(state: StateMultiModelo) -> StateMultiModelo:\n",
        "    \"\"\"\n",
        "    Usa GPT-4o-mini para an√°lisis t√©cnicos y comparaciones.\n",
        "    M√°s potente que GPT-3.5, ideal para razonamiento.\n",
        "    \"\"\"\n",
        "    print(\"\\nüü° [ADVANCED MODEL] Procesando con GPT-4o-mini...\")\n",
        "    \n",
        "    prompt = f\"\"\"Eres un experto t√©cnico en equipos de construcci√≥n de CONCESA.\n",
        "Proporciona un an√°lisis detallado y profesional.\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Incluye:\n",
        "1. An√°lisis t√©cnico\n",
        "2. Recomendaciones espec√≠ficas\n",
        "3. Consideraciones importantes\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "    \n",
        "    response = llm_advanced.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    print(f\"   ‚úÖ An√°lisis completo generado ({len(response.content)} chars)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"respuesta\": response.content,\n",
        "        \"modelo_usado\": \"gpt-4o-mini\",\n",
        "        \"requiere_validacion\": False\n",
        "    }\n",
        "\n",
        "# üî¥ NODO 4: Responder Cr√≠tico (GPT-4o)\n",
        "def responder_critico(state: StateMultiModelo) -> StateMultiModelo:\n",
        "    \"\"\"\n",
        "    Usa GPT-4o para consultas cr√≠ticas que requieren m√°xima calidad.\n",
        "    M√°s caro pero necesario para decisiones importantes.\n",
        "    \"\"\"\n",
        "    print(\"\\nüî¥ [PREMIUM MODEL] Procesando con GPT-4o (modelo premium)...\")\n",
        "    \n",
        "    prompt = f\"\"\"Eres un asesor senior de CONCESA con 20 a√±os de experiencia.\n",
        "Esta es una consulta CR√çTICA que requiere tu m√°xima atenci√≥n y expertise.\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Proporciona:\n",
        "1. An√°lisis exhaustivo\n",
        "2. Recomendaciones basadas en mejores pr√°cticas\n",
        "3. Consideraciones de riesgo\n",
        "4. Pr√≥ximos pasos sugeridos\n",
        "5. Informaci√≥n de contacto para seguimiento\n",
        "\n",
        "Esta respuesta ser√° revisada por un humano antes de enviarla.\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "    \n",
        "    response = llm_premium.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    print(f\"   ‚úÖ An√°lisis premium generado ({len(response.content)} chars)\")\n",
        "    print(\"   ‚ö†Ô∏è Requiere validaci√≥n humana\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"respuesta\": response.content,\n",
        "        \"modelo_usado\": \"gpt-4o\",\n",
        "        \"requiere_validacion\": True  # Casos cr√≠ticos siempre van a revisi√≥n\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ 4 nodos definidos, cada uno con su modelo especializado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Funci√≥n de Routing (Enrutamiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Router configurado\n"
          ]
        }
      ],
      "source": [
        "# üîÄ Funci√≥n que decide a qu√© nodo ir seg√∫n la complejidad\n",
        "def decidir_modelo(state: StateMultiModelo) -> Literal[\"simple\", \"complejo\", \"critico\"]:\n",
        "    \"\"\"\n",
        "    Routing inteligente basado en la complejidad detectada.\n",
        "    Esta funci√≥n determina el flujo del grafo.\n",
        "    \"\"\"\n",
        "    complejidad = state[\"complejidad\"]\n",
        "    \n",
        "    print(f\"\\nüîÄ [ROUTER] Decisi√≥n: '{complejidad}' ‚Üí\", end=\" \")\n",
        "    \n",
        "    if complejidad == \"SIMPLE\":\n",
        "        print(\"Modelo Simple (GPT-3.5)\")\n",
        "        return \"simple\"\n",
        "    elif complejidad == \"COMPLEJO\":\n",
        "        print(\"Modelo Avanzado (GPT-4o-mini)\")\n",
        "        return \"complejo\"\n",
        "    else:  # CRITICO\n",
        "        print(\"Modelo Premium (GPT-4o)\")\n",
        "        return \"critico\"\n",
        "\n",
        "print(\"‚úÖ Router configurado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Construir el Grafo Multi-Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Grafo Multi-Modelo compilado\n",
            "\n",
            "üï∏Ô∏è Arquitectura:\n",
            "\n",
            "        START\n",
            "          ‚îÇ\n",
            "          ‚ñº\n",
            "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "    ‚îÇ CLASIFICAR  ‚îÇ (GPT-3.5)\n",
            "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "          ‚îÇ\n",
            "    ¬øComplejidad?\n",
            "     ‚ï±    ‚îÇ    ‚ï≤\n",
            "    ‚ï±     ‚îÇ     ‚ï≤\n",
            " SIMPLE COMPLEJO CRITICO\n",
            "   ‚îÇ      ‚îÇ       ‚îÇ\n",
            "  3.5   4o-mini  4o\n",
            "   ‚îÇ      ‚îÇ       ‚îÇ\n",
            "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "          ‚îÇ\n",
            "         END\n"
          ]
        }
      ],
      "source": [
        "# üï∏Ô∏è Construir el grafo\n",
        "workflow_multi = StateGraph(StateMultiModelo)\n",
        "\n",
        "# Agregar nodos\n",
        "workflow_multi.add_node(\"clasificar\", clasificar_pregunta)\n",
        "workflow_multi.add_node(\"simple\", responder_simple)\n",
        "workflow_multi.add_node(\"complejo\", responder_complejo)\n",
        "workflow_multi.add_node(\"critico\", responder_critico)\n",
        "\n",
        "# Flujo: START ‚Üí clasificar ‚Üí [routing condicional] ‚Üí END\n",
        "workflow_multi.add_edge(START, \"clasificar\")\n",
        "\n",
        "# Edge condicional desde clasificar\n",
        "workflow_multi.add_conditional_edges(\n",
        "    \"clasificar\",\n",
        "    decidir_modelo,\n",
        "    {\n",
        "        \"simple\": \"simple\",\n",
        "        \"complejo\": \"complejo\",\n",
        "        \"critico\": \"critico\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Todos los modelos terminan\n",
        "workflow_multi.add_edge(\"simple\", END)\n",
        "workflow_multi.add_edge(\"complejo\", END)\n",
        "workflow_multi.add_edge(\"critico\", END)\n",
        "\n",
        "# Compilar\n",
        "app_multi = workflow_multi.compile()\n",
        "\n",
        "print(\"\\n‚úÖ Grafo Multi-Modelo compilado\")\n",
        "print(\"\\nüï∏Ô∏è Arquitectura:\")\n",
        "print(\"\")\n",
        "print(\"        START\")\n",
        "print(\"          ‚îÇ\")\n",
        "print(\"          ‚ñº\")\n",
        "print(\"    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "print(\"    ‚îÇ CLASIFICAR  ‚îÇ (GPT-3.5)\")\n",
        "print(\"    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "print(\"          ‚îÇ\")\n",
        "print(\"    ¬øComplejidad?\")\n",
        "print(\"     ‚ï±    ‚îÇ    ‚ï≤\")\n",
        "print(\"    ‚ï±     ‚îÇ     ‚ï≤\")\n",
        "print(\" SIMPLE COMPLEJO CRITICO\")\n",
        "print(\"   ‚îÇ      ‚îÇ       ‚îÇ\")\n",
        "print(\"  3.5   4o-mini  4o\")\n",
        "print(\"   ‚îÇ      ‚îÇ       ‚îÇ\")\n",
        "print(\"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "print(\"          ‚îÇ\")\n",
        "print(\"         END\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 üé¨ DEMO: Probar el Sistema Multi-Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üé¨ DEMO: SISTEMA MULTI-MODELO EN ACCI√ìN\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PRUEBA #1 - Se espera complejidad: SIMPLE\n",
            "================================================================================\n",
            "\n",
            "üë§ Pregunta: ¬øA qu√© hora abren?\n",
            "\n",
            "üîç [CLASSIFIER] Analizando: '¬øA qu√© hora abren?'\n",
            "   Complejidad detectada: SIMPLE\n",
            "\n",
            "üîÄ [ROUTER] Decisi√≥n: 'SIMPLE' ‚Üí Modelo Simple (GPT-3.5)\n",
            "\n",
            "üü¢ [SIMPLE MODEL] Procesando con GPT-3.5...\n",
            "   ‚úÖ Respuesta generada (71 chars)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä RESULTADO:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "ü§ñ Modelo usado: gpt-3.5-turbo\n",
            "üìè Complejidad detectada: SIMPLE\n",
            "‚ö†Ô∏è Requiere validaci√≥n humana: No\n",
            "\n",
            "üí¨ Respuesta:\n",
            "\n",
            "Nuestro horario de apertura es de lunes a viernes de 8:00 am a 5:00 pm.\n",
            "\n",
            "================================================================================\n",
            "PRUEBA #2 - Se espera complejidad: COMPLEJO\n",
            "================================================================================\n",
            "\n",
            "üë§ Pregunta: ¬øQu√© equipo de demolici√≥n me recomiendan para un edificio de 5 pisos con concreto armado?\n",
            "\n",
            "üîç [CLASSIFIER] Analizando: '¬øQu√© equipo de demolici√≥n me recomiendan para un edificio de 5 pisos con concreto armado?'\n",
            "   Complejidad detectada: COMPLEJO\n",
            "\n",
            "üîÄ [ROUTER] Decisi√≥n: 'COMPLEJO' ‚Üí Modelo Avanzado (GPT-4o-mini)\n",
            "\n",
            "üü° [ADVANCED MODEL] Procesando con GPT-4o-mini...\n",
            "   ‚úÖ An√°lisis completo generado (4437 chars)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä RESULTADO:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "ü§ñ Modelo usado: gpt-4o-mini\n",
            "üìè Complejidad detectada: COMPLEJO\n",
            "‚ö†Ô∏è Requiere validaci√≥n humana: No\n",
            "\n",
            "üí¨ Respuesta:\n",
            "\n",
            "### An√°lisis T√©cnico\n",
            "\n",
            "La demolici√≥n de un edificio de 5 pisos con concreto armado implica una serie de desaf√≠os t√©cnicos que deben ser abordados con cuidado. El concreto armado es un material robusto y duradero, lo que requiere equipos de demolici√≥n que sean capaces de manejar su resistencia y garantizar la seguridad durante el proceso.\n",
            "\n",
            "#### Factores a considerar:\n",
            "1. **Estructura del Edificio**: La configuraci√≥n del edificio, incluyendo su dise√±o estructural y los m√©todos de construcci√≥n utilizados, influye en la elecci√≥n del equipo. Es importante realizar un an√°lisis estructural previo para identificar puntos cr√≠ticos y zonas de debilidad.\n",
            "   \n",
            "2. **Normativas y Regulaciones**: Deben cumplirse las normativas locales de construcci√≥n y demolici√≥n, as√≠ como los est√°ndares de seguridad. Esto incluye la gesti√≥n de residuos y la protecci√≥n del medio ambiente.\n",
            "\n",
            "3. **Accesibilidad y Espacio**: La disponibilidad de espacio alrededor del edificio y la accesibilidad para los equipos son factores cruciales que pueden limitar o facilitar la elecci√≥n del equipo.\n",
            "\n",
            "4. **M√©todo de Demolici√≥n**: Dependiendo de la estrategia elegida (demolici√≥n controlada, fragmentaci√≥n, etc.), se requerir√°n diferentes tipos de maquinaria.\n",
            "\n",
            "### Recomendaciones Espec√≠ficas\n",
            "\n",
            "1. **Excavadoras con Martillos Hidr√°ulicos**:\n",
            "   - **Descripci√≥n**: Estas excavadoras est√°n equipadas con un martillo hidr√°ulico que permite romper el concreto de manera eficiente.\n",
            "   - **Ventajas**: Son vers√°tiles y pueden ser utilizadas en diferentes alturas, permitiendo un acceso f√°cil a las partes superiores del edificio. Adem√°s, son efectivas para manejar el concreto armado.\n",
            "   - **Modelo Recomendado**: CAT 320D con martillo hidr√°ulico, que ofrece un equilibrio entre potencia y maniobrabilidad.\n",
            "\n",
            "2. **Cortadoras de Concreto**:\n",
            "   - **Descripci√≥n**: Estas m√°quinas son ideales para realizar cortes precisos en las estructuras de concreto, facilitando la demolici√≥n controlada.\n",
            "   - **Ventajas**: Permiten minimizar el da√±o a las estructuras adyacentes y son √∫tiles para preparar secciones del edificio para su demolici√≥n.\n",
            "   - **Modelo Recomendado**: Husqvarna K970, que ofrece un excelente rendimiento en cortes profundos y precisos.\n",
            "\n",
            "3. **Gr√∫as con Ganchos de Demolici√≥n**:\n",
            "   - **Descripci√≥n**: Las gr√∫as equipadas con ganchos de demolici√≥n son √∫tiles para derribar secciones m√°s grandes de la estructura.\n",
            "   - **Ventajas**: Pueden trabajar en alturas significativas y son efectivas para demoler techos y pisos superiores.\n",
            "   - **Modelo Recomendado**: Liebherr LTM 1200, que ofrece gran capacidad de carga y alcance.\n",
            "\n",
            "4. **Bulldozers**:\n",
            "   - **Descripci√≥n**: Utilizados para limpiar y nivelar el terreno despu√©s de la demolici√≥n, as√≠ como para derribar estructuras m√°s peque√±as.\n",
            "   - **Ventajas**: Su potencia permite mover grandes vol√∫menes de escombros y preparar el sitio para la siguiente fase de construcci√≥n.\n",
            "   - **Modelo Recomendado**: CAT D6T, que combina potencia con eficiencia en el manejo de terrenos.\n",
            "\n",
            "### Consideraciones Importantes\n",
            "\n",
            "1. **Seguridad**: La seguridad es primordial. Aseg√∫rese de que todos los operadores est√©n capacitados y que se sigan las pr√°cticas de seguridad adecuadas. Esto incluye el uso de equipo de protecci√≥n personal (EPP) y la implementaci√≥n de barreras de seguridad.\n",
            "\n",
            "2. **Gesti√≥n de Residuos**: Planifique la gesti√≥n de residuos de manera eficiente. Esto incluye la separaci√≥n de materiales reciclables y la disposici√≥n adecuada de los escombros.\n",
            "\n",
            "3. **Impacto Ambiental**: Considere el impacto ambiental de la demolici√≥n. Utilice t√©cnicas que minimicen el ruido y la contaminaci√≥n del aire, y cumpla con las normativas ambientales locales.\n",
            "\n",
            "4. **Planificaci√≥n y Cronograma**: Desarrolle un plan detallado y un cronograma para la demolici√≥n. Esto ayudar√° a coordinar las actividades y a minimizar interrupciones en las operaciones circundantes.\n",
            "\n",
            "5. **Evaluaci√≥n de Riesgos**: Realice una evaluaci√≥n de riesgos antes de comenzar la demolici√≥n para identificar posibles peligros y establecer medidas de mitigaci√≥n.\n",
            "\n",
            "En resumen, la elecci√≥n del equipo de demolici√≥n para un edificio de 5 pisos con concreto armado debe basarse en un an√°lisis t√©cnico exhaustivo, considerando factores como la estructura, la normativa, la accesibilidad y el m√©todo de demolici√≥n. Las recomendaciones espec√≠ficas de maquinaria, junto con consideraciones de seguridad y gesti√≥n ambiental, garantizar√°n un proceso de demolici√≥n eficiente y seguro.\n",
            "\n",
            "================================================================================\n",
            "PRUEBA #3 - Se espera complejidad: CRITICO\n",
            "================================================================================\n",
            "\n",
            "üë§ Pregunta: Necesito cotizaci√≥n para un proyecto de construcci√≥n de 6 meses con m√∫ltiples equipos valorado en 150,000. ¬øQu√© garant√≠as y t√©rminos contractuales ofrecen?\n",
            "\n",
            "üîç [CLASSIFIER] Analizando: 'Necesito cotizaci√≥n para un proyecto de construcci√≥n de 6 meses con m√∫ltiples equipos valorado en 150,000. ¬øQu√© garant√≠as y t√©rminos contractuales ofrecen?'\n",
            "   Complejidad detectada: CRITICO\n",
            "\n",
            "üîÄ [ROUTER] Decisi√≥n: 'CRITICO' ‚Üí Modelo Premium (GPT-4o)\n",
            "\n",
            "üî¥ [PREMIUM MODEL] Procesando con GPT-4o (modelo premium)...\n",
            "   ‚úÖ An√°lisis premium generado (2877 chars)\n",
            "   ‚ö†Ô∏è Requiere validaci√≥n humana\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä RESULTADO:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "ü§ñ Modelo usado: gpt-4o\n",
            "üìè Complejidad detectada: CRITICO\n",
            "‚ö†Ô∏è Requiere validaci√≥n humana: S√≠\n",
            "\n",
            "üí¨ Respuesta:\n",
            "\n",
            "**An√°lisis Exhaustivo:**\n",
            "\n",
            "Para un proyecto de construcci√≥n de 6 meses valorado en 150,000, es fundamental estructurar un contrato que proteja tanto al cliente como a la empresa constructora. La cotizaci√≥n debe desglosar claramente los costos asociados a cada fase del proyecto, incluyendo mano de obra, materiales, equipos y cualquier otro gasto adicional. Tambi√©n es crucial establecer un cronograma detallado que indique hitos espec√≠ficos y fechas de entrega.\n",
            "\n",
            "**Recomendaciones Basadas en Mejores Pr√°cticas:**\n",
            "\n",
            "1. **Garant√≠as de Calidad:** Ofrecer garant√≠as sobre los materiales y la mano de obra utilizada. Esto podr√≠a incluir una garant√≠a de un a√±o para corregir defectos de construcci√≥n.\n",
            "\n",
            "2. **Cl√°usulas de Penalizaci√≥n:** Incluir cl√°usulas que penalicen los retrasos o incumplimientos de los plazos acordados, salvo en casos de fuerza mayor.\n",
            "\n",
            "3. **Revisiones y Aprobaciones:** Programar revisiones peri√≥dicas con el cliente para asegurar que el proyecto est√° alineado con sus expectativas. Esto podr√≠a ser mensual o por hitos completados.\n",
            "\n",
            "4. **Transparencia Financiera:** Proveer informes financieros peri√≥dicos para mantener al cliente informado sobre el estado del presupuesto del proyecto.\n",
            "\n",
            "**Consideraciones de Riesgo:**\n",
            "\n",
            "1. **Riesgos Clim√°ticos:** Considerar posibles retrasos debido a condiciones clim√°ticas adversas y tener un plan de contingencia.\n",
            "\n",
            "2. **Disponibilidad de Materiales:** Asegurar la disponibilidad de materiales necesarios para evitar retrasos, especialmente aquellos que puedan estar sujetos a escasez o fluctuaci√≥n de precios.\n",
            "\n",
            "3. **Cumplimiento Normativo:** Verificar que todas las licencias y permisos requeridos est√©n en orden antes de iniciar cualquier trabajo.\n",
            "\n",
            "**Pr√≥ximos Pasos Sugeridos:**\n",
            "\n",
            "1. **Reuni√≥n Inicial:** Programar una reuni√≥n con el cliente para discutir el alcance del proyecto, revisar la cotizaci√≥n detallada y ajustar cualquier aspecto necesario.\n",
            "\n",
            "2. **Revisi√≥n del Contrato:** Trabajar con el departamento legal para revisar y ajustar el contrato propuesto, asegurando que todas las garant√≠as y t√©rminos est√©n claramente definidos.\n",
            "\n",
            "3. **Selecci√≥n de Equipos:** Confirmar la disponibilidad de los equipos necesarios para el proyecto y realizar cualquier reserva anticipada.\n",
            "\n",
            "4. **Capacitaci√≥n y Seguridad:** Planificar sesiones de capacitaci√≥n para el personal en temas de seguridad y uso de equipos, asegurando que se cumplan todas las normativas de seguridad.\n",
            "\n",
            "**Informaci√≥n de Contacto para Seguimiento:**\n",
            "\n",
            "Para m√°s informaci√≥n o para discutir detalles adicionales sobre la cotizaci√≥n y los t√©rminos contractuales, por favor contacte a:\n",
            "\n",
            "- **Nombre:** [Tu Nombre]\n",
            "- **Tel√©fono:** [Tu Tel√©fono]\n",
            "- **Correo Electr√≥nico:** [Tu Correo Electr√≥nico]\n",
            "- **Oficina:** [Direcci√≥n de la Oficina]\n",
            "\n",
            "Estamos comprometidos en ofrecer un servicio de calidad y en trabajar estrechamente con usted para asegurar el √©xito de su proyecto.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "‚úÖ DEMO COMPLETADA\n",
            "================================================================================\n",
            "\n",
            "üí° Observa c√≥mo el sistema:\n",
            "   1. Clasific√≥ autom√°ticamente cada pregunta\n",
            "   2. Us√≥ el modelo apropiado para cada caso\n",
            "   3. Marc√≥ las respuestas cr√≠ticas para revisi√≥n humana\n",
            "   4. Optimiz√≥ costos usando modelos m√°s baratos cuando es suficiente\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üé¨ DEMO: SISTEMA MULTI-MODELO EN ACCI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Preparar preguntas de diferentes complejidades\n",
        "preguntas_test = [\n",
        "    {\n",
        "        \"pregunta\": \"¬øA qu√© hora abren?\",\n",
        "        \"esperado\": \"SIMPLE\"\n",
        "    },\n",
        "    {\n",
        "        \"pregunta\": \"¬øQu√© equipo de demolici√≥n me recomiendan para un edificio de 5 pisos con concreto armado?\",\n",
        "        \"esperado\": \"COMPLEJO\"\n",
        "    },\n",
        "    {\n",
        "        \"pregunta\": \"Necesito cotizaci√≥n para un proyecto de construcci√≥n de 6 meses con m√∫ltiples equipos valorado en 150,000. ¬øQu√© garant√≠as y t√©rminos contractuales ofrecen?\",\n",
        "        \"esperado\": \"CRITICO\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, test in enumerate(preguntas_test, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PRUEBA #{i} - Se espera complejidad: {test['esperado']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nüë§ Pregunta: {test['pregunta']}\")\n",
        "    \n",
        "    # Ejecutar el grafo\n",
        "    resultado = app_multi.invoke({\n",
        "        \"pregunta\": test['pregunta'],\n",
        "        \"complejidad\": \"\",\n",
        "        \"contexto_tecnico\": \"\",\n",
        "        \"respuesta\": \"\",\n",
        "        \"modelo_usado\": \"\",\n",
        "        \"tokens_estimados\": 0,\n",
        "        \"requiere_validacion\": False\n",
        "    })\n",
        "    \n",
        "    # Mostrar resultados\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"üìä RESULTADO:\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    print(f\"\\nü§ñ Modelo usado: {resultado['modelo_usado']}\")\n",
        "    print(f\"üìè Complejidad detectada: {resultado['complejidad']}\")\n",
        "    print(f\"‚ö†Ô∏è Requiere validaci√≥n humana: {'S√≠' if resultado['requiere_validacion'] else 'No'}\")\n",
        "    print(f\"\\nüí¨ Respuesta:\\n\")\n",
        "    print(resultado['respuesta'])\n",
        "\n",
        "print(f\"\\n\\n{'='*80}\")\n",
        "print(\"‚úÖ DEMO COMPLETADA\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nüí° Observa c√≥mo el sistema:\")\n",
        "print(\"   1. Clasific√≥ autom√°ticamente cada pregunta\")\n",
        "print(\"   2. Us√≥ el modelo apropiado para cada caso\")\n",
        "print(\"   3. Marc√≥ las respuestas cr√≠ticas para revisi√≥n humana\")\n",
        "print(\"   4. Optimiz√≥ costos usando modelos m√°s baratos cuando es suficiente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üê≥ PARTE 2: Integraci√≥n con Ollama (Modelos Locales)\n",
        "\n",
        "## ¬øPor qu√© Ollama?\n",
        "\n",
        "**Ollama** permite correr modelos de IA **localmente**, sin costo de API:\n",
        "\n",
        "### Ventajas:\n",
        "- ‚úÖ **Costo $0** (no pagas por tokens)\n",
        "- ‚úÖ **Privacidad total** (datos no salen de tu servidor)\n",
        "- ‚úÖ **Offline** (funciona sin internet)\n",
        "- ‚úÖ **Baja latencia** (si tienes GPU local)\n",
        "\n",
        "### Desventajas:\n",
        "- ‚ùå Requiere GPU (o es muy lento en CPU)\n",
        "- ‚ùå Modelos open-source son menos potentes que GPT-4\n",
        "- ‚ùå Consume recursos de tu servidor\n",
        "\n",
        "### Caso de uso ideal:\n",
        "```\n",
        "Clasificaci√≥n inicial ‚Üí Ollama (Llama 3) - GRATIS\n",
        "                          ‚îÇ\n",
        "        ¬øRequiere razonamiento avanzado?\n",
        "                 ‚îÇ              ‚îÇ\n",
        "               No              S√≠\n",
        "                 ‚îÇ              ‚îÇ\n",
        "           Ollama (gratis)   GPT-4 (pago)\n",
        "```\n",
        "\n",
        "**Ahorro:** Usar Ollama para 80% de consultas simples ‚Üí Reducir costos de API en 80%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Setup de Ollama con Docker\n",
        "\n",
        "Primero necesitas tener Ollama corriendo. Aqu√≠ est√°n las instrucciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ollama est√° corriendo\n",
            "\n",
            "üì¶ Modelos disponibles: 1\n",
            "   - llama3.2:latest\n"
          ]
        }
      ],
      "source": [
        "# Este c√≥digo verifica si Ollama est√° corriendo\n",
        "import requests\n",
        "\n",
        "def verificar_ollama():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "        if response.status_code == 200:\n",
        "            modelos = response.json().get('models', [])\n",
        "            print(\"‚úÖ Ollama est√° corriendo\")\n",
        "            print(f\"\\nüì¶ Modelos disponibles: {len(modelos)}\")\n",
        "            for modelo in modelos:\n",
        "                print(f\"   - {modelo['name']}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Ollama responde pero con error\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Ollama NO est√° corriendo\")\n",
        "        print(\"\\nüìã Para iniciar Ollama, ejecuta:\")\n",
        "        print(\"\\n   OPCI√ìN 1 - Con Docker (recomendado):\")\n",
        "        print(\"   docker run -d -p 11434:11434 --name ollama ollama/ollama\")\n",
        "        print(\"   docker exec -it ollama ollama pull llama3.2\")\n",
        "        print(\"\\n   OPCI√ìN 2 - Instalaci√≥n nativa:\")\n",
        "        print(\"   Descarga desde https://ollama.com\")\n",
        "        print(\"   ollama pull llama3.2\")\n",
        "        return False\n",
        "\n",
        "ollama_disponible = verificar_ollama()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Configurar LangChain con Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ollama configurado\n",
            "\n",
            "üß™ Prueba r√°pida:\n",
            "   Respuesta: \"Hola, funciona correctamente\" es una frase que se puede traducir al ingl√©s como \"Hello, it works correctly\".\n",
            "\n",
            "üí° Ahora puedes usar llm_local como cualquier otro LLM de LangChain\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Solo configurar si Ollama est√° disponible\n",
        "if ollama_disponible:\n",
        "    # Modelo local con Ollama\n",
        "    llm_local = ChatOllama(\n",
        "        model=\"llama3.2\",  # o \"mistral\", \"codellama\", etc.\n",
        "        base_url=\"http://localhost:11434\",\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Ollama configurado\")\n",
        "    print(\"\\nüß™ Prueba r√°pida:\")\n",
        "    \n",
        "    # Test simple\n",
        "    response = llm_local.invoke([HumanMessage(content=\"Di 'Hola, funciono correctamente' en espa√±ol\")])\n",
        "    print(f\"   Respuesta: {response.content}\")\n",
        "    \n",
        "    print(\"\\nüí° Ahora puedes usar llm_local como cualquier otro LLM de LangChain\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Saltando configuraci√≥n de Ollama\")\n",
        "    print(\"   Puedes continuar con los modelos de OpenAI\")\n",
        "    llm_local = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Grafo H√≠brido: Ollama + OpenAI\n",
        "\n",
        "El mejor de ambos mundos:\n",
        "- **Ollama** para tareas simples (gratis, r√°pido)\n",
        "- **OpenAI** para tareas complejas (calidad premium)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Grafo H√≠brido (Ollama + OpenAI) creado\n",
            "\n",
            "üí∞ Estrategia de ahorro:\n",
            "   - Preguntas simples ‚Üí Ollama ($0)\n",
            "   - Preguntas complejas ‚Üí GPT-4o-mini ($$$)\n",
            "   - Ahorro potencial: 70-90% en costos de API\n"
          ]
        }
      ],
      "source": [
        "if ollama_disponible:\n",
        "    \n",
        "    class StateHibrido(TypedDict):\n",
        "        pregunta: str\n",
        "        es_simple: bool\n",
        "        respuesta: str\n",
        "        modelo_usado: str\n",
        "        costo_estimado: float  # En USD\n",
        "    \n",
        "    # Nodo 1: Clasificar con Ollama (gratis)\n",
        "    def clasificar_con_ollama(state: StateHibrido) -> StateHibrido:\n",
        "        print(\"\\nü¶ô [OLLAMA] Clasificando pregunta (GRATIS)...\")\n",
        "        \n",
        "        prompt = f\"\"\"Clasifica esta pregunta como SIMPLE o COMPLEJA.\n",
        "SIMPLE: Preguntas sobre horarios, ubicaci√≥n, contacto, stock b√°sico.\n",
        "COMPLEJA: An√°lisis t√©cnico, comparaciones, recomendaciones, c√°lculos.\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Responde SOLO: SIMPLE o COMPLEJA\"\"\"\n",
        "        \n",
        "        response = llm_local.invoke([HumanMessage(content=prompt)])\n",
        "        es_simple = \"SIMPLE\" in response.content.upper()\n",
        "        \n",
        "        print(f\"   Clasificaci√≥n: {'SIMPLE' if es_simple else 'COMPLEJA'}\")\n",
        "        \n",
        "        return {**state, \"es_simple\": es_simple, \"costo_estimado\": 0.0}\n",
        "    \n",
        "    # Nodo 2: Responder con Ollama\n",
        "    def responder_con_ollama(state: StateHibrido) -> StateHibrido:\n",
        "        print(\"\\nü¶ô [OLLAMA] Generando respuesta (GRATIS)...\")\n",
        "        \n",
        "        prompt = f\"\"\"Eres un asistente de CONCESA. Responde brevemente.\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "        \n",
        "        response = llm_local.invoke([HumanMessage(content=prompt)])\n",
        "        \n",
        "        print(\"   ‚úÖ Respuesta generada\")\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"respuesta\": response.content,\n",
        "            \"modelo_usado\": \"Llama 3.2 (local)\",\n",
        "            \"costo_estimado\": 0.0\n",
        "        }\n",
        "    \n",
        "    # Nodo 3: Responder con GPT-4o-mini\n",
        "    def responder_con_openai(state: StateHibrido) -> StateHibrido:\n",
        "        print(\"\\nü§ñ [OPENAI] Generando respuesta con GPT-4o-mini...\")\n",
        "        \n",
        "        prompt = f\"\"\"Eres un experto t√©cnico de CONCESA. Proporciona an√°lisis detallado.\n",
        "\n",
        "Pregunta: {state['pregunta']}\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "        \n",
        "        response = llm_advanced.invoke([HumanMessage(content=prompt)])\n",
        "        \n",
        "        # Estimar costo (rough)\n",
        "        tokens_input = len(prompt) // 4\n",
        "        tokens_output = len(response.content) // 4\n",
        "        costo = (tokens_input * 0.15 / 1_000_000) + (tokens_output * 0.60 / 1_000_000)\n",
        "        \n",
        "        print(f\"   ‚úÖ Respuesta generada (costo estimado: ${costo:.6f})\")\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"respuesta\": response.content,\n",
        "            \"modelo_usado\": \"GPT-4o-mini (API)\",\n",
        "            \"costo_estimado\": costo\n",
        "        }\n",
        "    \n",
        "    # Routing\n",
        "    def decidir_modelo_hibrido(state: StateHibrido) -> Literal[\"ollama\", \"openai\"]:\n",
        "        if state[\"es_simple\"]:\n",
        "            print(\"\\nüîÄ [ROUTER] ‚Üí Ollama (gratis)\")\n",
        "            return \"ollama\"\n",
        "        else:\n",
        "            print(\"\\nüîÄ [ROUTER] ‚Üí OpenAI (calidad premium)\")\n",
        "            return \"openai\"\n",
        "    \n",
        "    # Construir grafo h√≠brido\n",
        "    workflow_hibrido = StateGraph(StateHibrido)\n",
        "    \n",
        "    workflow_hibrido.add_node(\"clasificar\", clasificar_con_ollama)\n",
        "    workflow_hibrido.add_node(\"ollama\", responder_con_ollama)\n",
        "    workflow_hibrido.add_node(\"openai\", responder_con_openai)\n",
        "    \n",
        "    workflow_hibrido.add_edge(START, \"clasificar\")\n",
        "    workflow_hibrido.add_conditional_edges(\n",
        "        \"clasificar\",\n",
        "        decidir_modelo_hibrido,\n",
        "        {\"ollama\": \"ollama\", \"openai\": \"openai\"}\n",
        "    )\n",
        "    workflow_hibrido.add_edge(\"ollama\", END)\n",
        "    workflow_hibrido.add_edge(\"openai\", END)\n",
        "    \n",
        "    app_hibrido = workflow_hibrido.compile()\n",
        "    \n",
        "    print(\"\\n‚úÖ Grafo H√≠brido (Ollama + OpenAI) creado\")\n",
        "    print(\"\\nüí∞ Estrategia de ahorro:\")\n",
        "    print(\"   - Preguntas simples ‚Üí Ollama ($0)\")\n",
        "    print(\"   - Preguntas complejas ‚Üí GPT-4o-mini ($$$)\")\n",
        "    print(\"   - Ahorro potencial: 70-90% en costos de API\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ollama no disponible, saltando configuraci√≥n h√≠brida\")\n",
        "    app_hibrido = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 üé¨ DEMO: Sistema H√≠brido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üé¨ DEMO: SISTEMA H√çBRIDO (Ollama + OpenAI)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "CONSULTA #1\n",
            "================================================================================\n",
            "\n",
            "üë§ Pregunta: ¬øTienen rotomartillos disponibles?\n",
            "\n",
            "ü¶ô [OLLAMA] Clasificando pregunta (GRATIS)...\n",
            "   Clasificaci√≥n: SIMPLE\n",
            "\n",
            "üîÄ [ROUTER] ‚Üí Ollama (gratis)\n",
            "\n",
            "ü¶ô [OLLAMA] Generando respuesta (GRATIS)...\n",
            "   ‚úÖ Respuesta generada\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä RESULTADO:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "ü§ñ Modelo: Llama 3.2 (local)\n",
            "üí∞ Costo: $0.000000\n",
            "\n",
            "üí¨ Respuesta:\n",
            "\n",
            "S√≠, tenemos rotomartillos disponibles en nuestras tiendas y en nuestro sitio web. ¬øNecesitas ayuda para encontrar uno?\n",
            "\n",
            "================================================================================\n",
            "CONSULTA #2\n",
            "================================================================================\n",
            "\n",
            "üë§ Pregunta: Necesito comparar las especificaciones t√©cnicas de sus demoledores m√°s potentes para un proyecto de demolici√≥n de un edificio de 8 pisos\n",
            "\n",
            "ü¶ô [OLLAMA] Clasificando pregunta (GRATIS)...\n",
            "   Clasificaci√≥n: COMPLEJA\n",
            "\n",
            "üîÄ [ROUTER] ‚Üí OpenAI (calidad premium)\n",
            "\n",
            "ü§ñ [OPENAI] Generando respuesta con GPT-4o-mini...\n",
            "   ‚úÖ Respuesta generada (costo estimado: $0.000546)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä RESULTADO:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "ü§ñ Modelo: GPT-4o-mini (API)\n",
            "üí∞ Costo: $0.000546\n",
            "\n",
            "üí¨ Respuesta:\n",
            "\n",
            "Para realizar un an√°lisis detallado de los demoledores m√°s potentes de CONCESA, es importante considerar varios aspectos t√©cnicos que influir√°n en su efectividad para un proyecto de demolici√≥n de un edificio de 8 pisos. A continuaci√≥n, se presentan las especificaciones clave que debes tener en cuenta al comparar los modelos disponibles:\n",
            "\n",
            "### 1. **Potencia y Rendimiento**\n",
            "   - **Potencia del Motor**: La potencia del motor, medida en kilovatios (kW) o caballos de fuerza (HP), es fundamental. Modelos con motores m√°s potentes generalmente ofrecen un rendimiento superior en tareas de demolici√≥n.\n",
            "   - **Energ√≠a de Impacto**: Medida en julios (J), esta especificaci√≥n indica la energ√≠a que el demoledor puede aplicar en cada golpe. Modelos con mayor energ√≠a de impacto son m√°s eficaces para romper concreto y estructuras de gran resistencia.\n",
            "\n",
            "### 2. **Peso y Dise√±o**\n",
            "   - **Peso del Demoledor**: Un mayor peso puede proporcionar m√°s fuerza de impacto, pero tambi√©n puede afectar la maniobrabilidad. Es esencial encontrar un equilibrio entre peso y control.\n",
            "   - **Dise√±o Ergon√≥mico**: Un dise√±o que facilite el manejo y reduzca la fatiga del operador es crucial para proyectos prolongados.\n",
            "\n",
            "### 3. **Tipo de Herramienta**\n",
            "   - **Puntas y Accesorios**: La disponibilidad de diferentes puntas (como puntas de cincel, de ruptura o de perforaci√≥n) permite adaptar el demoledor a diferentes aplicaciones en la demolici√≥n.\n",
            "   - **Compatibilidad con Equipos**: Aseg√∫rate de que el demoledor sea compatible con otros equipos de construcci√≥n que puedas tener en el sitio.\n",
            "\n",
            "### 4. **Sistema de Amortiguaci√≥n**\n",
            "   - **Reducci√≥n de Vibraciones**: Un sistema de amortiguaci√≥n efectivo puede reducir la vibraci√≥n transmitida al operador, aumentando la comodidad y seguridad durante su uso.\n",
            "\n",
            "### 5. **Eficiencia Energ√©tica**\n",
            "   - **Consumo de Combustible o Energ√≠a**: Eval√∫a la eficiencia del consumo de combustible (si es un modelo a gasolina o di√©sel) o la eficiencia energ√©tica (en modelos el√©ctricos) para minimizar los costos operativos.\n",
            "\n",
            "### 6. **Facilidad de Mantenimiento**\n",
            "   - **Accesibilidad de Piezas**: Un dise√±o que facilite el mantenimiento y la reparaci√≥n puede reducir el tiempo de inactividad y aumentar la productividad.\n",
            "   - **Durabilidad**: Materiales de alta calidad y dise√±o robusto son esenciales para resistir el uso intensivo en condiciones dif√≠ciles.\n",
            "\n",
            "### 7. **Seguridad**\n",
            "   - **Caracter√≠sticas de Seguridad**: Examina las caracter√≠sticas de seguridad, como sistemas de apagado autom√°tico, protecci√≥n contra sobrecalentamiento y dispositivos de seguridad para el operador.\n",
            "\n",
            "### Ejemplo de Modelos Potentes de CONCESA\n",
            "1. **Demoledor Modelo A**\n",
            "   - Potencia: 15 kW\n",
            "   - Energ√≠a de Impacto: 200 J\n",
            "   - Peso: 40 kg\n",
            "   - Sistema de Amortiguaci√≥n: S√≠\n",
            "   - Consumo: 5 L/h (di√©sel)\n",
            "\n",
            "2. **Demoledor Modelo B**\n",
            "   - Potencia: 18 kW\n",
            "   - Energ√≠a de Impacto: 250 J\n",
            "   - Peso: 45 kg\n",
            "   - Sistema de Amortiguaci√≥n: S√≠\n",
            "   - Consumo: 6 L/h (di√©sel)\n",
            "\n",
            "3. **Demoledor Modelo C**\n",
            "   - Potencia: 20 kW\n",
            "   - Energ√≠a de Impacto: 300 J\n",
            "   - Peso: 50 kg\n",
            "   - Sistema de Amortiguaci√≥n: S√≠\n",
            "   - Consumo: 4 L/h (el√©ctrico)\n",
            "\n",
            "### Conclusi√≥n\n",
            "Para un proyecto de demolici√≥n de un edificio de 8 pisos, es recomendable optar por un modelo que ofrezca alta potencia, energ√≠a de impacto significativa y un dise√±o que favorezca la maniobrabilidad y la seguridad. Considera tambi√©n el tipo de material a demoler y las condiciones del sitio para seleccionar el demoledor que mejor se adapte a tus necesidades. Si tienes m√°s preguntas o necesitas informaci√≥n espec√≠fica sobre un modelo, no dudes en consultarlo.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üí∞ AN√ÅLISIS DE COSTOS\n",
            "================================================================================\n",
            "\n",
            "Costo total de las 2 consultas: $0.000546\n",
            "\n",
            "üîç Comparaci√≥n si us√°ramos SOLO GPT-4o para todo:\n",
            "   Costo estimado: ~$0.001638 (3x m√°s caro)\n",
            "\n",
            "‚úÖ Ahorro con sistema h√≠brido: ~67%\n"
          ]
        }
      ],
      "source": [
        "if app_hibrido:\n",
        "    print(\"=\"*80)\n",
        "    print(\"üé¨ DEMO: SISTEMA H√çBRIDO (Ollama + OpenAI)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    preguntas_hibrido = [\n",
        "        \"¬øTienen rotomartillos disponibles?\",  # SIMPLE ‚Üí Ollama\n",
        "        \"Necesito comparar las especificaciones t√©cnicas de sus demoledores m√°s potentes para un proyecto de demolici√≥n de un edificio de 8 pisos\"  # COMPLEJA ‚Üí OpenAI\n",
        "    ]\n",
        "    \n",
        "    costo_total = 0.0\n",
        "    \n",
        "    for i, pregunta in enumerate(preguntas_hibrido, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"CONSULTA #{i}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"\\nüë§ Pregunta: {pregunta}\")\n",
        "        \n",
        "        resultado = app_hibrido.invoke({\n",
        "            \"pregunta\": pregunta,\n",
        "            \"es_simple\": False,\n",
        "            \"respuesta\": \"\",\n",
        "            \"modelo_usado\": \"\",\n",
        "            \"costo_estimado\": 0.0\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{'‚îÄ'*80}\")\n",
        "        print(\"üìä RESULTADO:\")\n",
        "        print(f\"{'‚îÄ'*80}\")\n",
        "        print(f\"\\nü§ñ Modelo: {resultado['modelo_usado']}\")\n",
        "        print(f\"üí∞ Costo: ${resultado['costo_estimado']:.6f}\")\n",
        "        print(f\"\\nüí¨ Respuesta:\\n\")\n",
        "        print(resultado['respuesta'])\n",
        "        \n",
        "        costo_total += resultado['costo_estimado']\n",
        "    \n",
        "    print(f\"\\n\\n{'='*80}\")\n",
        "    print(\"üí∞ AN√ÅLISIS DE COSTOS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nCosto total de las {len(preguntas_hibrido)} consultas: ${costo_total:.6f}\")\n",
        "    print(f\"\\nüîç Comparaci√≥n si us√°ramos SOLO GPT-4o para todo:\")\n",
        "    print(f\"   Costo estimado: ~${costo_total * 3:.6f} (3x m√°s caro)\")\n",
        "    print(f\"\\n‚úÖ Ahorro con sistema h√≠brido: ~{((1 - costo_total/(costo_total*3)) * 100):.0f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Demo h√≠brida omitida (Ollama no disponible)\")\n",
        "    print(\"\\nPuedes instalar Ollama para ver esta funcionalidad:\")\n",
        "    print(\"https://ollama.com\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üìö PARTE 3: Resumen y Mejores Pr√°cticas\n",
        "\n",
        "## ‚úÖ Lo que aprendiste:\n",
        "\n",
        "### 1. Multi-Modelo Strategy\n",
        "- ‚úÖ Usar diferentes modelos para diferentes tareas\n",
        "- ‚úÖ Clasificaci√≥n autom√°tica de complejidad\n",
        "- ‚úÖ Routing inteligente entre modelos\n",
        "- ‚úÖ Optimizaci√≥n de costos\n",
        "\n",
        "### 2. Ollama Integration\n",
        "- ‚úÖ Configurar modelos locales con Docker\n",
        "- ‚úÖ Integrar Ollama con LangChain\n",
        "- ‚úÖ Sistema h√≠brido (local + cloud)\n",
        "- ‚úÖ Reducir costos con modelos gratis\n",
        "\n",
        "## üí° Mejores Pr√°cticas:\n",
        "\n",
        "### Cu√°ndo usar cada modelo:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Tarea           ‚îÇ Modelo       ‚îÇ Raz√≥n                   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Clasificaci√≥n   ‚îÇ GPT-3.5      ‚îÇ R√°pido, barato          ‚îÇ\n",
        "‚îÇ FAQ simple      ‚îÇ Ollama/3.5   ‚îÇ Costo $0 o m√≠nimo       ‚îÇ\n",
        "‚îÇ An√°lisis        ‚îÇ GPT-4o-mini  ‚îÇ Balance costo/calidad   ‚îÇ\n",
        "‚îÇ Cr√≠tico         ‚îÇ GPT-4o       ‚îÇ M√°xima calidad          ‚îÇ\n",
        "‚îÇ C√≥digo          ‚îÇ GPT-4o       ‚îÇ Precisi√≥n necesaria     ‚îÇ\n",
        "‚îÇ Embeddings      ‚îÇ text-3-small ‚îÇ M√°s barato para RAG     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Estrategia de ahorro:\n",
        "\n",
        "1. **80/20 Rule**: 80% de consultas son simples ‚Üí Usa modelos baratos\n",
        "2. **Clasificaci√≥n primero**: Invierte poco en clasificar, ahorra mucho despu√©s\n",
        "3. **Validaci√≥n humana**: Casos cr√≠ticos siempre van a revisi√≥n\n",
        "4. **Cach√© respuestas**: Para FAQs comunes\n",
        "5. **Ollama para dev**: Desarrollo local sin costo\n",
        "\n",
        "## üöÄ Pr√≥ximos pasos:\n",
        "\n",
        "En la siguiente secci√≥n veremos:\n",
        "1. **Persistencia con checkpoints** (guardar progreso del grafo)\n",
        "2. **Human-in-the-loop avanzado** (aprobaciones multi-nivel)\n",
        "3. **Streaming** (respuestas en tiempo real)\n",
        "4. **Monitoreo y observability** (LangSmith)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Ejercicios Propuestos\n",
        "\n",
        "## Ejercicio 1: Agregar un cuarto nivel\n",
        "Modifica el grafo multi-modelo para agregar un nivel \"ULTRA-CR√çTICO\" que:\n",
        "- Use GPT-4o\n",
        "- Requiera validaci√≥n de 2 humanos\n",
        "- Genere un reporte PDF\n",
        "\n",
        "## Ejercicio 2: Implementar cach√©\n",
        "Agrega un nodo de \"cache\" que:\n",
        "- Verifique si la pregunta ya fue respondida antes\n",
        "- Si existe, retorne la respuesta cacheada (sin usar API)\n",
        "- Si no existe, proceda al flujo normal\n",
        "\n",
        "## Ejercicio 3: Modelo de embeddings local\n",
        "Integra un modelo de embeddings local con Ollama para:\n",
        "- B√∫squeda sem√°ntica sin costo\n",
        "- Comparar resultados vs OpenAI embeddings\n",
        "- Medir diferencia en calidad\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Contin√∫a a la siguiente parte para ver Docker Compose y configuraci√≥n de producci√≥n!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "curos-ia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
