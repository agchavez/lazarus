{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó LangChain Avanzado - Agentes y Herramientas\n",
    "## Clase 2 - Lazarus\n",
    "\n",
    "En esta clase profundizaremos en:\n",
    "1. **Chains**: Cadenas personalizadas y composici√≥n\n",
    "2. **Memory**: Gesti√≥n de memoria conversacional\n",
    "3. **Agents**: Agentes aut√≥nomos con herramientas\n",
    "4. **Tools**: Creaci√≥n de herramientas personalizadas\n",
    "5. **Output Parsers**: Estructuraci√≥n de respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Instalaci√≥n de dependencias\n",
    "\n",
    "Ejecuta esta celda una sola vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install openai python-dotenv langchain langchain-openai langchain-community \\\n    langchain-core langchain-classic pypdf faiss-cpu wikipedia duckduckgo-search requests -q"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargamos variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Verificamos la API key\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è No se encontr√≥ OPENAI_API_KEY en el archivo .env\")\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n cargada correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üîó PARTE 1: Chains (Cadenas)\n",
    "\n",
    "Las **Chains** son secuencias de componentes que procesan datos de forma encadenada.\n",
    "\n",
    "### Tipos de Chains:\n",
    "- **LLMChain**: B√°sica (prompt + LLM)\n",
    "- **SequentialChain**: Encadena m√∫ltiples chains\n",
    "- **RouterChain**: Enruta a diferentes chains seg√∫n la entrada\n",
    "- **TransformChain**: Transforma datos antes de pasar al LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 LLMChain B√°sica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import PromptTemplate\n\n# Configuramos el LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# Creamos un prompt template\nprompt = PromptTemplate(\n    input_variables=[\"producto\", \"caracteristicas\"],\n    template=\"\"\"Eres un copywriter experto. Crea una descripci√≥n de producto atractiva.\n\nProducto: {producto}\nCaracter√≠sticas: {caracteristicas}\n\nDescripci√≥n persuasiva:\"\"\"\n)\n\n# Creamos la chain usando LCEL (LangChain Expression Language)\nchain_descripcion = prompt | llm\n\n# Ejecutamos\nresultado = chain_descripcion.invoke({\n    \"producto\": \"Rotomartillo HILTI TE-70\",\n    \"caracteristicas\": \"Potente, ideal para concreto, marca profesional\"\n})\n\nprint(\"üéØ DESCRIPCI√ìN GENERADA:\")\nprint(resultado.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 SequentialChain - Cadenas en Secuencia\n",
    "\n",
    "Encadenamos m√∫ltiples operaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.output_parsers import StrOutputParser\n\n# Chain 1: Generar caracter√≠sticas t√©cnicas\nprompt1 = PromptTemplate(\n    input_variables=[\"producto\"],\n    template=\"Lista 5 caracter√≠sticas t√©cnicas clave de: {producto}\"\n)\nchain1 = prompt1 | llm | StrOutputParser()\n\n# Chain 2: Crear descripci√≥n de venta\nprompt2 = PromptTemplate(\n    input_variables=[\"caracteristicas\"],\n    template=\"\"\"Convierte estas caracter√≠sticas t√©cnicas en un pitch de ventas atractivo:\n{caracteristicas}\n\nPitch (m√°ximo 50 palabras):\"\"\"\n)\nchain2 = prompt2 | llm | StrOutputParser()\n\n# Combinamos las chains usando LCEL\nfrom operator import itemgetter\n\ncadena_completa = (\n    {\"caracteristicas\": chain1}\n    | chain2\n)\n\n# Ejecutamos\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîÑ Ejecutando cadena secuencial...\")\nresultado_final = cadena_completa.invoke({\"producto\": \"Compactador de Rodillo 524 KGS\"})\nprint(\"\\nüéØ RESULTADO FINAL:\")\nprint(resultado_final)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üí≠ PARTE 2: Memory (Memoria Conversacional)\n",
    "\n",
    "La **memoria** permite que el agente recuerde conversaciones anteriores.\n",
    "\n",
    "### Tipos de Memoria:\n",
    "- **ConversationBufferMemory**: Almacena todo el historial\n",
    "- **ConversationBufferWindowMemory**: Solo las √∫ltimas K conversaciones\n",
    "- **ConversationSummaryMemory**: Resume conversaciones antiguas\n",
    "- **ConversationEntityMemory**: Recuerda entidades mencionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Memory B√°sica - Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_community.chat_message_histories import ChatMessageHistory\n\n# Creamos memoria\nstore = {}\n\ndef get_session_history(session_id: str):\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n# Creamos el prompt con memoria\nprompt_memoria = ChatPromptTemplate.from_messages([\n    (\"system\", \"Eres un asistente amable y profesional.\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\n\n# Creamos la chain con memoria\nchain_con_memoria = prompt_memoria | llm\n\nconversacion = RunnableWithMessageHistory(\n    chain_con_memoria,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"\n)\n\n# Simulamos una conversaci√≥n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üó£Ô∏è CONVERSACI√ìN CON MEMORIA\")\nprint(\"=\"*60)\n\nsession_id = \"sesion_1\"\n\nresp1 = conversacion.invoke(\n    {\"input\": \"Hola, me llamo Juan y necesito rentar un demoledor\"},\n    config={\"configurable\": {\"session_id\": session_id}}\n)\nprint(f\"\\nü§ñ Respuesta 1: {resp1.content}\")\n\nresp2 = conversacion.invoke(\n    {\"input\": \"¬øCu√°l me recomiendas para concreto pesado?\"},\n    config={\"configurable\": {\"session_id\": session_id}}\n)\nprint(f\"\\nü§ñ Respuesta 2: {resp2.content}\")\n\nresp3 = conversacion.invoke(\n    {\"input\": \"¬øRecuerdas mi nombre?\"},\n    config={\"configurable\": {\"session_id\": session_id}}\n)\nprint(f\"\\nü§ñ Respuesta 3: {resp3.content}\")\n\n# Inspeccionamos la memoria\nprint(\"\\nüìù HISTORIAL DE CONVERSACI√ìN:\")\nhistory = get_session_history(session_id)\nfor msg in history.messages:\n    print(f\"  {msg.type}: {msg.content[:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Window Memory - Solo las √∫ltimas K conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\n# Memoria con ventana (solo √∫ltimas K mensajes)\nstore_ventana = {}\n\ndef get_window_history(session_id: str):\n    if session_id not in store_ventana:\n        store_ventana[session_id] = ChatMessageHistory()\n    return store_ventana[session_id]\n\n# Helper para mantener solo √∫ltimos K mensajes\ndef trim_messages(messages, k=4):  # k=4 significa 2 interacciones (2 humano + 2 AI)\n    return messages[-k:]\n\nprompt_ventana = ChatPromptTemplate.from_messages([\n    (\"system\", \"Eres un asistente amable.\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\n\nchain_ventana = prompt_ventana | llm\n\nconversacion_ventana = RunnableWithMessageHistory(\n    chain_ventana,\n    get_window_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ü™ü CONVERSACI√ìN CON VENTANA DE MEMORIA (k=2)\")\nprint(\"=\"*60)\n\nsession_id_2 = \"sesion_ventana\"\n\nconversacion_ventana.invoke(\n    {\"input\": \"Me llamo Mar√≠a\"},\n    config={\"configurable\": {\"session_id\": session_id_2}}\n)\n\nconversacion_ventana.invoke(\n    {\"input\": \"Vivo en Honduras\"},\n    config={\"configurable\": {\"session_id\": session_id_2}}\n)\n\nconversacion_ventana.invoke(\n    {\"input\": \"Necesito un compresor\"},\n    config={\"configurable\": {\"session_id\": session_id_2}}\n)\n\n# Trimear mensajes manualmente\nhistory = get_window_history(session_id_2)\nhistory.messages = trim_messages(history.messages, k=4)\n\n# Esta pregunta debe fallar porque \"Mar√≠a\" ya sali√≥ de la ventana\nresp = conversacion_ventana.invoke(\n    {\"input\": \"¬øRecuerdas mi nombre?\"},\n    config={\"configurable\": {\"session_id\": session_id_2}}\n)\nprint(f\"\\nü§ñ {resp.content}\")\n\nprint(\"\\nüí° Solo recuerda las √∫ltimas 2 interacciones, por eso no recuerda el nombre\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Summary Memory - Resume conversaciones largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_community.chat_message_histories import ChatMessageHistory\n\n# Para Summary Memory, necesitamos simular el resumen manualmente\n# ya que ConversationSummaryMemory est√° deprecado\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìÑ CONVERSACI√ìN CON RESUMEN AUTOM√ÅTICO\")\nprint(\"=\"*60)\n\n# Simulamos conversaciones\nconversaciones = [\n    {\"humano\": \"Soy Pedro, gerente de construcci√≥n de CONCESA\", \"ai\": \"Mucho gusto Pedro, ¬øen qu√© puedo ayudarte hoy?\"},\n    {\"humano\": \"Estamos construyendo un edificio de 5 pisos\", \"ai\": \"Excelente proyecto. ¬øQu√© tipo de equipos necesitas?\"},\n    {\"humano\": \"Necesitamos equipos para 3 meses\", \"ai\": \"Perfecto, tenemos opciones de renta a largo plazo con descuentos.\"},\n    {\"humano\": \"El presupuesto es limitado\", \"ai\": \"Entiendo, podemos buscar las opciones m√°s econ√≥micas para ti.\"}\n]\n\n# Creamos un prompt para resumir\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt_resumen = ChatPromptTemplate.from_messages([\n    (\"system\", \"Resume la siguiente conversaci√≥n de forma concisa:\"),\n    (\"human\", \"{conversacion}\")\n])\n\nchain_resumen = prompt_resumen | llm | StrOutputParser()\n\n# Convertir conversaciones a texto\ntexto_conversacion = \"\\n\".join([\n    f\"Cliente: {c['humano']}\\nAsistente: {c['ai']}\" \n    for c in conversaciones\n])\n\n# Generar resumen\nresumen = chain_resumen.invoke({\"conversacion\": texto_conversacion})\n\nprint(\"\\nüìä RESUMEN DE LA CONVERSACI√ìN:\")\nprint(resumen)\n\nprint(\"\\nüí° El LLM resumi√≥ toda la conversaci√≥n en vez de almacenar todo\")\nprint(\"\\nüìù Conversaci√≥n original:\")\nfor i, c in enumerate(conversaciones, 1):\n    print(f\"{i}. Cliente: {c['humano']}\")\n    print(f\"   Asistente: {c['ai']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ñ PARTE 3: Agents (Agentes Aut√≥nomos)\n",
    "\n",
    "Los **Agents** son sistemas que pueden:\n",
    "- Decidir qu√© herramientas usar\n",
    "- Razonar sobre problemas\n",
    "- Ejecutar acciones de forma aut√≥noma\n",
    "\n",
    "### Tipos de Agentes:\n",
    "- **Zero-shot React**: Decide herramientas sin ejemplos\n",
    "- **Conversational**: Con memoria conversacional\n",
    "- **OpenAI Functions**: Usa function calling de OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Crear Herramientas Personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.tools import tool\nfrom datetime import datetime\n\n# Usamos el decorador @tool para crear herramientas (forma moderna)\n\n@tool\ndef calcular_descuento(precio_dias: str) -> str:\n    \"\"\"Calcula descuento por volumen. Input: 'precio,dias' ejemplo: '500,10'\"\"\"\n    try:\n        precio, dias = precio_dias.split(',')\n        precio = float(precio)\n        dias = int(dias)\n        \n        total = precio * dias\n        \n        # Descuentos por volumen\n        if dias >= 30:\n            descuento = 0.20\n        elif dias >= 14:\n            descuento = 0.15\n        elif dias >= 7:\n            descuento = 0.10\n        else:\n            descuento = 0\n        \n        total_con_descuento = total * (1 - descuento)\n        ahorro = total - total_con_descuento\n        \n        return f\"Total sin descuento: L{total:.2f}\\nDescuento aplicado: {descuento*100}%\\nTotal con descuento: L{total_con_descuento:.2f}\\nAhorro: L{ahorro:.2f}\"\n    except:\n        return \"Error: Formato debe ser 'precio,dias' ejemplo: '500,10'\"\n\n@tool\ndef verificar_disponibilidad(equipo: str) -> str:\n    \"\"\"Verifica si un equipo est√° disponible. Input: nombre del equipo\"\"\"\n    # Base de datos simulada\n    inventario = {\n        \"demoledor\": {\"disponible\": True, \"unidades\": 3},\n        \"rotomartillo\": {\"disponible\": True, \"unidades\": 5},\n        \"compactador\": {\"disponible\": False, \"unidades\": 0},\n        \"mezcladora\": {\"disponible\": True, \"unidades\": 2}\n    }\n    \n    equipo_lower = equipo.lower()\n    for key in inventario:\n        if key in equipo_lower:\n            info = inventario[key]\n            if info[\"disponible\"]:\n                return f\"‚úÖ {equipo} est√° DISPONIBLE. Unidades en stock: {info['unidades']}\"\n            else:\n                return f\"‚ùå {equipo} NO est√° disponible actualmente. Stock: {info['unidades']}\"\n    \n    return f\"‚ö†Ô∏è No encontr√© informaci√≥n sobre '{equipo}' en el inventario\"\n\n@tool\ndef calcular_fecha_entrega(dias: str) -> str:\n    \"\"\"Calcula la fecha de entrega. Input: n√∫mero de d√≠as de renta\"\"\"\n    try:\n        from datetime import timedelta\n        dias = int(dias)\n        fecha_entrega = datetime.now() + timedelta(days=dias)\n        return f\"Si rentas por {dias} d√≠as, la fecha de devoluci√≥n ser√≠a: {fecha_entrega.strftime('%d/%m/%Y')}\"\n    except:\n        return \"Error: Ingresa un n√∫mero v√°lido de d√≠as\"\n\nprint(\"‚úÖ Herramientas personalizadas creadas con decorador @tool\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Crear el Agente con Herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_classic.agents import create_react_agent, AgentExecutor\nfrom langchain_core.prompts import PromptTemplate\n\n# Las herramientas ya est√°n creadas con el decorador @tool en la celda anterior\ntools = [\n    calcular_descuento,\n    verificar_disponibilidad,\n    calcular_fecha_entrega\n]\n\n# Template para el agente ReAct\ntemplate = '''Responde a las siguientes preguntas lo mejor que puedas. Tienes acceso a las siguientes herramientas:\n\n{tools}\n\nUsa el siguiente formato:\n\nQuestion: la pregunta de entrada que debes responder\nThought: siempre debes pensar qu√© hacer\nAction: la acci√≥n a tomar, debe ser una de [{tool_names}]\nAction Input: el input para la acci√≥n\nObservation: el resultado de la acci√≥n\n... (este Thought/Action/Action Input/Observation puede repetirse N veces)\nThought: Ahora s√© la respuesta final\nFinal Answer: la respuesta final a la pregunta de entrada original\n\nComienza!\n\nQuestion: {input}\nThought: {agent_scratchpad}'''\n\nprompt_agente = PromptTemplate.from_template(template)\n\n# Crear el agente\nagente = create_react_agent(llm, tools, prompt_agente)\n\n# Crear el executor\nagente_executor = AgentExecutor(\n    agent=agente,\n    tools=tools,\n    verbose=True,\n    max_iterations=5,\n    handle_parsing_errors=True\n)\n\nprint(\"\\n‚úÖ Agente creado con 3 herramientas usando ReAct\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Demo del Agente en Acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"ü§ñ AGENTE AUT√ìNOMO EN ACCI√ìN\")\nprint(\"=\"*60)\n\n# Pregunta 1: Verificar disponibilidad\nprint(\"\\nüë§ Usuario: ¬øTienen rotomartillos disponibles?\")\nresp1 = agente_executor.invoke({\"input\": \"¬øTienen rotomartillos disponibles?\"})\nprint(f\"ü§ñ Agente: {resp1['output']}\")\n\n# Pregunta 2: Calcular precio con descuento\nprint(\"\\nüë§ Usuario: ¬øCu√°nto me costar√≠a rentar un demoledor por 15 d√≠as a L550 por d√≠a?\")\nresp2 = agente_executor.invoke({\"input\": \"¬øCu√°nto me costar√≠a rentar un demoledor por 15 d√≠as a L550 por d√≠a?\"})\nprint(f\"ü§ñ Agente: {resp2['output']}\")\n\n# Pregunta 3: Fecha de entrega\nprint(\"\\nüë§ Usuario: Si lo rento por 15 d√≠as, ¬øcu√°ndo lo tengo que devolver?\")\nresp3 = agente_executor.invoke({\"input\": \"Si lo rento por 15 d√≠as, ¬øcu√°ndo lo tengo que devolver?\"})\nprint(f\"ü§ñ Agente: {resp3['output']}\")\n\nprint(\"\\nüí° El agente decidi√≥ autom√°ticamente qu√© herramientas usar para cada pregunta\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üõ†Ô∏è PARTE 4: Output Parsers (Estructurar Respuestas)\n",
    "\n",
    "Los **Output Parsers** permiten que el LLM devuelva datos estructurados (JSON, listas, objetos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Structured Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom pydantic import BaseModel, Field\n\n# Definimos el esquema con Pydantic\nclass ProductoSchema(BaseModel):\n    nombre_producto: str = Field(description=\"Nombre del producto\")\n    precio: str = Field(description=\"Precio por d√≠a en Lempiras\")\n    categoria: str = Field(description=\"Categor√≠a del producto\")\n    recomendado_para: str = Field(description=\"Para qu√© tipo de trabajo es recomendado\")\n\n# Creamos el parser\noutput_parser = JsonOutputParser(pydantic_object=ProductoSchema)\n\n# Creamos el prompt\nprompt = PromptTemplate(\n    template=\"\"\"Extrae la informaci√≥n del siguiente producto y devu√©lvela en formato JSON.\n\n{format_instructions}\n\nProducto: {producto}\n\"\"\",\n    input_variables=[\"producto\"],\n    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n)\n\n# Creamos la chain\nchain_estructurada = prompt | llm | output_parser\n\n# Ejecutamos\nresultado = chain_estructurada.invoke({\n    \"producto\": \"Demoledor TE-3000, martillo rompedor excepcional para demolici√≥n pesada de concreto, L1,100.00 por d√≠a\"\n})\n\nprint(\"\\nüìä SALIDA ESTRUCTURADA (JSON):\")\nprint(resultado)\nprint(\"\\nüí° Ahora podemos usar estos datos program√°ticamente\")\nprint(f\"   Nombre: {resultado['nombre_producto']}\")\nprint(f\"   Precio: {resultado['precio']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pydantic Output Parser (Validaci√≥n con Tipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n# Definimos el modelo con Pydantic\nclass Producto(BaseModel):\n    nombre: str = Field(description=\"Nombre del producto\")\n    precio: float = Field(description=\"Precio por d√≠a en Lempiras\")\n    caracteristicas: List[str] = Field(description=\"Lista de caracter√≠sticas principales\")\n    disponible: bool = Field(description=\"Si est√° disponible o no\")\n\n# Creamos el parser\nparser = PydanticOutputParser(pydantic_object=Producto)\n\n# Creamos el prompt\nprompt = PromptTemplate(\n    template=\"\"\"Analiza este producto y devuelve la informaci√≥n estructurada.\n\n{format_instructions}\n\nDescripci√≥n: {descripcion}\n\"\"\",\n    input_variables=[\"descripcion\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\n# Chain\nchain_pydantic = prompt | llm | parser\n\n# Ejecutamos\nproducto = chain_pydantic.invoke({\n    \"descripcion\": \"\"\"Rotomartillo TE-70 HILTI, martillo perforador muy potente para \n    tareas de carga pesada de taladro y cincelado en concreto. Precio L750 por d√≠a. \n    Actualmente disponible en stock.\"\"\"\n})\n\nprint(\"\\nüéØ OBJETO PYDANTIC VALIDADO:\")\nprint(f\"Nombre: {producto.nombre}\")\nprint(f\"Precio: L{producto.precio}\")\nprint(f\"Caracter√≠sticas: {producto.caracteristicas}\")\nprint(f\"Disponible: {'S√≠' if producto.disponible else 'No'}\")\n\nprint(\"\\nüí° Pydantic valida autom√°ticamente los tipos de datos\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ PARTE 5: Proyecto Final - Agente Completo de Ventas\n",
    "\n",
    "Combinamos todo lo aprendido en un agente de ventas inteligente con:\n",
    "- ‚úÖ Memoria conversacional\n",
    "- ‚úÖ Herramientas personalizadas\n",
    "- ‚úÖ RAG para consultar cat√°logo\n",
    "- ‚úÖ Output estructurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.tools import tool\nfrom langchain_classic.agents import create_react_agent, AgentExecutor\n\nprint(\"üèóÔ∏è CONSTRUYENDO AGENTE DE VENTAS COMPLETO...\\n\")\n\n# 1. Cargamos el cat√°logo PDF (del notebook anterior)\nPDF_PATH = 'Documentos - PDF/Catalogo_Equipos_Construccion.pdf'\n\nif os.path.exists(PDF_PATH):\n    print(\"üìö Cargando cat√°logo PDF...\")\n    loader = PyPDFLoader(PDF_PATH)\n    documents = loader.load()\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500,\n        chunk_overlap=100\n    )\n    docs = text_splitter.split_documents(documents)\n    \n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n    vectorstore = FAISS.from_documents(docs, embeddings)\n    \n    print(f\"‚úÖ Cat√°logo cargado: {len(docs)} chunks\")\n    \n    # Creamos herramienta de b√∫squeda en cat√°logo usando decorador @tool\n    @tool\n    def buscar_catalogo(query: str) -> str:\n        \"\"\"Busca productos en el cat√°logo. Input: descripci√≥n del producto\"\"\"\n        docs = vectorstore.similarity_search(query, k=2)\n        resultados = \"\\n\\n\".join([doc.page_content for doc in docs])\n        return f\"Productos encontrados:\\n{resultados}\"\n    \n    # Actualizamos las herramientas\n    tools_completo = tools + [buscar_catalogo]\n    \n    # Template mejorado para agente de ventas\n    template_ventas = '''Eres un agente de ventas experto de CONCESA que ayuda a clientes a encontrar equipos de construcci√≥n.\nSiempre eres amable, profesional y buscas cerrar ventas.\n\nTienes acceso a las siguientes herramientas:\n\n{tools}\n\nUsa el siguiente formato:\n\nQuestion: la pregunta del cliente\nThought: siempre piensa qu√© necesita el cliente\nAction: la acci√≥n a tomar, debe ser una de [{tool_names}]\nAction Input: el input para la acci√≥n\nObservation: el resultado de la acci√≥n\n... (puedes repetir Thought/Action/Action Input/Observation varias veces)\nThought: Ahora tengo la informaci√≥n completa para responder\nFinal Answer: tu respuesta profesional y √∫til al cliente\n\nComienza!\n\nQuestion: {input}\nThought: {agent_scratchpad}'''\n\n    prompt_ventas = PromptTemplate.from_template(template_ventas)\n    \n    # Crear agente de ventas\n    agente_ventas_react = create_react_agent(llm, tools_completo, prompt_ventas)\n    \n    agente_ventas = AgentExecutor(\n        agent=agente_ventas_react,\n        tools=tools_completo,\n        verbose=True,\n        max_iterations=5,\n        handle_parsing_errors=True\n    )\n    \n    print(\"‚úÖ Agente de ventas completo creado con 4 herramientas\")\nelse:\n    print(\"‚ö†Ô∏è No se encontr√≥ el PDF. El agente funcionar√° sin cat√°logo.\")\n    agente_ventas = agente_executor  # Usamos el agente anterior"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Demo Final - Agente de Ventas Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"üé¨ DEMO FINAL - AGENTE DE VENTAS INTELIGENTE\")\nprint(\"=\"*70)\n\n# Simulamos una conversaci√≥n de ventas completa\nconversacion_demo = [\n    \"Hola, necesito equipos para demoler concreto\",\n    \"¬øCu√°l es el m√°s potente que tienen?\",\n    \"Perfecto, lo necesito por 20 d√≠as. ¬øCu√°nto ser√≠a el total con el TE-3000?\",\n    \"¬øEst√° disponible para entrega inmediata?\"\n]\n\nfor i, pregunta in enumerate(conversacion_demo, 1):\n    print(f\"\\nüë§ Cliente: {pregunta}\")\n    respuesta = agente_ventas.invoke({\"input\": pregunta})\n    print(f\"\\nü§ñ Agente: {respuesta['output']}\")\n    print(\"\\n\" + \"-\"*70)\n\nprint(\"\\n‚ú® El agente us√≥ autom√°ticamente:\")\nprint(\"   1. B√∫squeda en cat√°logo (RAG)\")\nprint(\"   2. C√°lculo de descuentos\")\nprint(\"   3. Verificaci√≥n de disponibilidad\")\nprint(\"   4. Razonamiento paso a paso (ReAct)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìö Resumen de Conceptos Clave\n",
    "\n",
    "## Lo que aprendimos hoy:\n",
    "\n",
    "### 1. **Chains** (Cadenas)\n",
    "- Encadenar operaciones de forma secuencial\n",
    "- Componer workflows complejos\n",
    "- `LLMChain`, `SequentialChain`\n",
    "\n",
    "### 2. **Memory** (Memoria)\n",
    "- Mantener contexto conversacional\n",
    "- Diferentes estrategias: Buffer, Window, Summary\n",
    "- Fundamental para chatbots\n",
    "\n",
    "### 3. **Agents** (Agentes)\n",
    "- Sistemas aut√≥nomos que deciden qu√© hacer\n",
    "- Usan herramientas seg√∫n la necesidad\n",
    "- Razonamiento ReAct (Reason + Act)\n",
    "\n",
    "### 4. **Tools** (Herramientas)\n",
    "- Extender capacidades del LLM\n",
    "- Herramientas personalizadas\n",
    "- Integraci√≥n con APIs, bases de datos, etc.\n",
    "\n",
    "### 5. **Output Parsers**\n",
    "- Estructurar respuestas del LLM\n",
    "- JSON, Pydantic, listas\n",
    "- Validaci√≥n de tipos\n",
    "\n",
    "### 6. **Integraci√≥n RAG + Agents**\n",
    "- Combinar b√∫squeda vectorial con agentes\n",
    "- Sistema completo de atenci√≥n al cliente\n",
    "- Escalable y mantenible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üöÄ Ejercicio Final\n",
    "\n",
    "**Reto**: Crea tu propio agente personalizado que:\n",
    "1. Tenga al menos 2 herramientas personalizadas\n",
    "2. Use memoria conversacional\n",
    "3. Devuelva salidas estructuradas\n",
    "\n",
    "Ideas:\n",
    "- Agente de soporte t√©cnico\n",
    "- Asistente de investigaci√≥n\n",
    "- Generador de reportes\n",
    "- Planificador de proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë®‚Äçüíª TU C√ìDIGO AQU√ç\n",
    "# Crea tu agente personalizado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìñ Recursos Adicionales\n",
    "\n",
    "- [Documentaci√≥n LangChain](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [LangChain Agents](https://python.langchain.com/docs/modules/agents/)\n",
    "- [LangChain Memory](https://python.langchain.com/docs/modules/memory/)\n",
    "- [LangChain Tools](https://python.langchain.com/docs/modules/tools/)\n",
    "- [Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "\n",
    "---\n",
    "## ‚úÖ Fin de la Clase 2\n",
    "\n",
    "**¬øPreguntas?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}